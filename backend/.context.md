# 后端模块上下文

## 模块概览
后端模块是 MetaPrompter 的核心逻辑部分，负责处理前端请求、管理数据、集成 LLM 服务，并提供 RESTful API。

## 技术栈
- **框架**: Django 5.2.6
- **API 框架**: Django REST Framework
- **数据库**: 开发环境使用 SQLite，生产环境计划使用 MySQL
- **ORM**: Django ORM
- **LLM 框架**: LangChain 0.2.16，OpenAI Python SDK 1.40.6
- **依赖管理**: uv + pyproject.toml
- **环境变量管理**: python-dotenv

## 目录结构
```
backend/
├── metaprompter/       # 主项目目录
│   ├── __init__.py     # 包初始化文件
│   ├── settings.py     # 项目设置（使用环境变量）
│   ├── urls.py         # 主 URL 配置
│   ├── wsgi.py         # WSGI 配置
│   └── asgi.py         # ASGI 配置
├── prompt/             # Prompt 管理应用
│   ├── __init__.py     # 包初始化文件
│   ├── admin.py        # 后台管理配置
│   ├── apps.py         # 应用配置
│   ├── migrations/     # 数据库迁移文件
│   ├── models.py       # 数据模型（Prompt, PromptHistory, PromptStatistics）
│   ├── serializers.py  # DRF 序列化器
│   ├── tests.py        # 测试文件
│   ├── urls.py         # 应用 URL 配置
│   ├── views.py        # DRF 视图集
│   └── llm_utils/      # LLM 相关工具和服务
│       ├── __init__.py # 包初始化文件
│       ├── config.py   # LLM 配置和客户端初始化
│       ├── template_manager.py # 提示词模板管理
│       └── service.py  # LLM 服务核心功能实现
├── .env                # 环境变量配置文件
├── .context.md         # 后端模块上下文文件
├── manage.py           # Django 管理脚本
├── pyproject.toml      # 项目依赖和配置
└── README.md           # 项目说明文档
```

## 核心功能
### 已实现的功能
1. **Prompt 管理**: 
   - 创建、编辑、保存、删除 Prompt 的完整 CRUD API
   - 支持公开/私有 Prompt 设置
   - 支持标签管理

2. **历史记录**: 
   - 设计并实现了 PromptHistory 数据模型
   - 提供只读 API 用于查看历史记录
   - 支持记录 generate、evaluate 等操作类型

3. **统计分析**: 
   - 设计并实现了 PromptStatistics 数据模型
   - 提供只读 API 用于查看统计数据
   - 支持记录 generation_count 等统计指标

4. **LLM 集成**: 
   - 集成 LangChain 0.2.16 框架
   - 集成 OpenAI Python SDK 1.40.6
   - 在 prompt 应用中创建了 llm_utils 子目录专门用于存放 LLM 相关代码
   - 实现 LLM 服务封装和调用管理
   - 设计并实现 LLM 客户端配置和初始化机制
   - 实现提示词模板管理系统

5. **Prompt 生成**: 
   - 基于需求描述的智能 Prompt 生成
   - 集成到现有的 Prompt 管理 API 中
   - 支持指定模型参数

6. **Prompt 优化**: 
   - 分析现有 Prompt 结构和质量
   - 提供具体的优化建议
   - 集成到现有的 Prompt 管理 API 中

7. **Prompt 评估**: 
   - 评估提示词质量
   - 提供详细或简要的分析报告
   - 集成到现有的 Prompt 管理 API 中

### 计划实现的功能
8. **本地 LLM 模型支持**: 
   - 添加对本地部署的 LLM 模型支持
   - 实现模型选择和切换机制

9. **模型选择与切换功能**: 
   - 提供用户友好的模型选择界面
   - 实现模型间无缝切换

10. **Prompt 结构深度分析**: 
    - 分析提示词的组成结构和要素
    - 提供详细的结构优化建议

11. **高级数据分析功能**: 
    - 实现更复杂的统计分析
    - 提供可视化数据展示

12. **复杂提示词链式执行**: 
    - 支持多个提示词按顺序执行
    - 实现提示词间的数据传递

13. **提示词模板库扩展**: 
    - 扩充常用提示词模板
    - 提供自定义模板功能

## 数据库模型
### Prompt 模型
- **字段**: id, name, description, content, created_at, updated_at, created_by, is_public, tags
- **关系**: 一对多关系到 User（created_by）；与 PromptHistory 一对多；与 PromptStatistics 一对一
- **功能**: 存储提示词的基本信息和内容

### PromptHistory 模型
- **字段**: id, prompt, content, operation_type, user, created_at
- **关系**: 一对多关系到 Prompt 和 User
- **功能**: 记录 Prompt 的操作历史
- **操作类型**: create, update, use, optimize, generate, evaluate

### PromptStatistics 模型
- **字段**: id, prompt, total_usage, daily_usage, weekly_usage, monthly_usage, total_score, score_count, optimization_count, average_optimization_improvement, generation_count, updated_at
- **关系**: 一对一关系到 Prompt
- **功能**: 存储 Prompt 的使用和优化统计数据

## API 设计
采用 RESTful 设计风格，主要 API 端点如下:

### 1. Prompt 管理 API
- **GET /api/prompts/**
  - 功能：获取所有 Prompt 列表
  - 参数：可选分页、搜索、排序参数
  - 返回：Prompt 列表数据

- **POST /api/prompts/**
  - 功能：创建新的 Prompt
  - 参数：name（名称）、content（内容）、description（描述）、is_public（是否公开）、tags（标签）等
  - 返回：创建成功的 Prompt 详情

- **GET /api/prompts/{id}/**
  - 功能：获取单个 Prompt 详情
  - 参数：id（Prompt ID）
  - 返回：Prompt 详情数据

- **PUT /api/prompts/{id}/**
  - 功能：更新 Prompt
  - 参数：id（Prompt ID）及更新的字段
  - 返回：更新后的 Prompt 详情

- **DELETE /api/prompts/{id}/**
  - 功能：删除 Prompt
  - 参数：id（Prompt ID）
  - 返回：成功状态

### 2. Prompt 操作 API
- **POST /api/prompts/generate/**
  - 功能：根据用户需求生成新的提示词
  - 参数：requirement（需求描述）、name（提示词名称，可选）、description（提示词描述，可选）、model（LLM模型，可选）
  - 返回：生成的提示词详情

- **POST /api/prompts/{id}/use/**
  - 功能：标记提示词被使用
  - 参数：无
  - 返回：成功状态

- **POST /api/prompts/{id}/optimize/**
  - 功能：优化现有提示词
  - 参数：model（LLM模型，可选）
  - 返回：优化后的提示词内容

- **POST /api/prompts/{id}/evaluate/**
  - 功能：评估提示词质量
  - 参数：detailed（是否详细评估，默认true）、model（LLM模型，可选）
  - 返回：评估结果和分析报告

### 3. 公共 Prompt API
- **GET /api/public-prompts/**
  - 功能：获取所有公开的 Prompt 列表
  - 返回：公开 Prompt 列表数据

### 4. 历史记录 API
- **GET /api/histories/**
  - 功能：获取操作历史列表
  - 参数：可选分页、过滤参数
  - 返回：历史记录列表数据

- **GET /api/histories/{id}/**
  - 功能：获取单个操作历史详情
  - 参数：id（历史记录 ID）
  - 返回：历史记录详情数据

### 5. 统计分析 API
- **GET /api/statistics/**
  - 功能：获取统计数据列表
  - 参数：可选分页、过滤参数
  - 返回：统计数据列表

- **GET /api/statistics/{id}/**
  - 功能：获取单个统计数据详情
  - 参数：id（统计数据 ID）
  - 返回：统计数据详情

## 序列化器
实现了以下序列化器：
- **PromptSerializer**: 用于 Prompt 模型的完整序列化
- **PromptCreateSerializer**: 专用于创建 Prompt 的序列化器
- **PromptUpdateSerializer**: 专用于更新 Prompt 的序列化器
- **PromptHistorySerializer**: 用于 PromptHistory 模型的序列化
- **PromptStatisticsSerializer**: 用于 PromptStatistics 模型的序列化

## 视图集
实现了以下视图集：
- **PromptViewSet**: 提供 Prompt 模型的完整 CRUD 操作，包含额外的自定义动作：generate, use, optimize, evaluate
- **PromptHistoryViewSet**: 提供 PromptHistory 模型的只读操作
- **PromptStatisticsViewSet**: 提供 PromptStatistics 模型的只读操作
- **PublicPromptListView**: 提供获取公开 Prompt 的列表视图

## LLM 集成详情
### LLM 工具目录结构
```
prompt/llm_utils/
├── __init__.py     # 包初始化文件，导出主要服务实例
├── config.py       # LLM 配置和客户端初始化
├── template_manager.py # 提示词模板管理
└── service.py      # LLM 服务核心功能实现
```

### 核心服务类
1. **PromptGenerationService**: 负责根据用户需求生成新的提示词
   - 支持从需求描述生成提示词
   - 支持从模板生成提示词
   - 提供参数化配置选项

2. **PromptOptimizationService**: 负责优化现有提示词
   - 分析提示词结构和质量
   - 生成优化建议
   - 支持多种优化策略

3. **LLMEvaluationService**: 负责评估提示词质量
   - 提供详细和简要两种评估模式
   - 分析提示词的清晰度、有效性、完整性等指标
   - 生成结构化的评估报告

## 环境变量配置
主要环境变量包括：
- SECRET_KEY: Django 密钥
- DEBUG: 调试模式开关
- ALLOWED_HOSTS: 允许的主机名
- DATABASE_ENGINE: 数据库引擎
- DATABASE_NAME: 数据库名称
- DATABASE_USER: 数据库用户名
- DATABASE_PASSWORD: 数据库密码
- DATABASE_HOST: 数据库主机
- DATABASE_PORT: 数据库端口
- LANGUAGE_CODE: 语言代码
- TIME_ZONE: 时区设置
- OPENAI_API_KEY: OpenAI API 密钥
- OPENAI_API_BASE: OpenAI API 基础 URL
- DEFAULT_LLM_MODEL: 默认 LLM 模型名称

## 开发规范
- 遵循 Django 项目结构最佳实践
- 每个功能模块独立成应用
- 使用 Django ORM 进行数据库操作
- API 设计遵循 RESTful 原则
- 代码风格统一，使用 Black 和 Flake8
- 提交代码前进行测试